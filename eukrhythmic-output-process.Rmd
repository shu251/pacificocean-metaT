---
title: "eukrhythmic-output-process"
output: html_document
date: '2023-04-10'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up environment
```{r}
# Must install in this order
# library(multidplyr)
library(tidyverse)
# WHOI: installing ggplot2, dplyr, tidyr? r version 4.2.2
# library(dbplyr)
# library(RSQLite)
# library(tictoc) #use for timing
# library(vroom)
library(data.table)
library(edgeR)
#
num_threads <- getDTthreads()
```


# Import count table by contigs

Uncomment and only run on HPC.
```{r}
# tic()
# counts_all <- fread("../../../scratch/user/skhu/SPOT-ALOHA/03-abundance_tables/ReadTable_ByContig.csv", 
#                  verbose = TRUE,
#                  drop = 1,
#                  showProgress = TRUE,
#                  nThread = num_threads)
# toc() # 59.262 sec elapsed
```

```{r}
# tic()
# annot_all <- fread("../../../scratch/user/skhu/SPOT-ALOHA/02-annotation_table/TaxonomicAndFunctionalAnnotations.csv", 
#                  verbose = TRUE,
#                  showProgress = TRUE,
#                  nThread = num_threads,
#                  header = TRUE)
# toc() # 7.046 sec elapsed
# ?fread()
```

## Import metadata

```{r}
tic()
metadata <- fread("input-data/complete-sample-list.txt", 
                 # verbose = TRUE,
                 showProgress = TRUE,
                 nThread = num_threads,
                 header = TRUE)
toc() # 0.042 sec elapsed
```

```{r}
head(metadata)

# Order of current names of table
srr_order <- data.frame(RUN = (names(counts_all)[2:44]))

# set re-name from metadata, include RUN srr IDs
srr_list_rename <- select(metadata, SAMPLENAME, RUN) %>% 
  distinct() %>% 
  unite(SRR_EDGER, SAMPLENAME, RUN, sep = "-", remove = FALSE)

# Re-order these based on table
joined_reorder <- srr_order %>% 
  left_join(srr_list_rename)

# All should be equal to TRUE
srr_order == as.character(joined_reorder$RUN)

# Set new names
srr_rename <- as.character(joined_reorder$SRR_EDGER)
```

Rename counts table
```{r}
# srr_rename
names(counts_all)
colnames(counts_all)[2:44] <- srr_rename
names(counts_all)
```

### QC read counts with what is annotated

What sequences were not annotated? Remove them from the data.

```{r}
# ID seqID associated 
head(counts_all$ShortSeqID) # There are more of these here, than was annotated

# What is total number of ShortSeqIDs?
tic()
length(unique(counts_all$ShortSeqID)); toc()
# 54,080,665
```
There are over 54 million hits of read on contigs.

```{r}
# head(annot_all)
tic()
length(unique(annot_all$ShortSeqID)); toc() #14,737,135 # 14 million with annotations

tic()
length(unique(annot_all$SequenceID)); toc()
# 14840771 - 14737135
# 103,636 equate to duplicated ORFs
```


Of these reads, only 14 million had an annotation. This is 27% of the reads that will be used in downstream analysis.

```{r}
100*(14840771/54080665)
```

### Remove unannotated.
```{r}
seqIDs_wannot <- as.character(unique(annot_all$ShortSeqID))

# Save unannotated
unannot_counts <- counts_all %>% 
  filter(!(ShortSeqID %in% seqIDs_wannot))

counts_annot <- counts_all %>% 
  filter(ShortSeqID %in% seqIDs_wannot)


```

New counts table.
```{r}
dim(counts_annot)
```



# EdgeR library normalization
Automate text for edgeR input
```{r}
# head(srr_list_rename)
srr_edgeR <- srr_list_rename %>% 
  group_by(SAMPLENAME) %>% 
  mutate(NUM = n()) %>% 
  arrange(SAMPLENAME)

edger_text <- srr_edgeR %>% 
  select(SAMPLENAME, NUM) %>% 
  distinct() %>% 
  mutate(TEXT = paste(
    'rep("', SAMPLENAME,'",',NUM,')',sep = ""))

# edit(edger_text$TEXT)
# ?edit()

reorder_col_names <- as.character(srr_edgeR$SRR_EDGER)
reorder_col_names_alpha <- c("ShortSeqID","SequenceID", reorder_col_names)
reorder_col_names_alpha
typeof(reorder_col_names_alpha)
# dim(counts_annot)
# length(reorder_col_names_alpha)
tic()
counts_annot_reorder <- annot_all %>% 
  select(ShortSeqID, SequenceID) %>% 
  left_join(counts_annot) %>% 
  select(reorder_col_names_alpha) %>% 
  # column_to_rownames(var = "SequenceID") %>%
  as.matrix
toc()

# save(counts_annot_reorder, file = "/scratch/user/skhu/SPOT-ALOHA/tmp-may6-2023.RData")
```
```{r}
load(file = "/scratch/user/skhu/SPOT-ALOHA/tmp-may6-2023.RData", verbose = TRUE)
```


```{r}
head(counts_annot_reorder)[,1:4]
str(counts_annot_reorder)
dim(counts_annot_reorder)
# 3:45 should be numeric
```
At this point, I have short seq IDs, long seq IDs, and read counts for all the samples as columns. 


```{r}
tic()
all_counts_df <- as.data.frame(counts_annot_reorder) %>% 
  mutate(row_id = row_number()) %>% 
  unite("READ_ID_FULL", c("ShortSeqID", "row_id"), sep = ";") %>% # make unique
  select(-SequenceID) %>% 
  column_to_rownames(var = "READ_ID_FULL") %>% 
  mutate_if(is.character, as.numeric)
toc() #69.541 sec elapsed
```
```{r}
str(all_counts_df)
head(all_counts_df)[,1:4]

# all_counts_mat <- as.matr
```


### EdgeR replicate names as groups

```{r}
# group = c(rep("CA_Catalina_5_May",6),
#           rep("CA_PortofLA_5_May",6),
#           rep("CA_SPOT_150_May",3),
#           rep("CA_SPOT_5_May",12),
#           rep("CA_SPOT_890_May",4),
#           rep("NPSG_ALOHA_1000_July",1),
#           rep("NPSG_ALOHA_1000_March",1),
#           rep("NPSG_ALOHA_119_July",2),
#           rep("NPSG_ALOHA_120_March",2),
#           rep("NPSG_ALOHA_150_July",1),
#           rep("NPSG_ALOHA_150_March",1),
#           rep("NPSG_ALOHA_5_July",2),
#           rep("NPSG_ALOHA_5_March",2))
```


## EdgeR

Prep matrix of counts
```{r}
head(all_counts_matrix)
y <- dim(all_counts_matrix)[2]
y # should be 43
# ?DGEList()

# annot_genes <- annot_all %>% 
  # distinct()
  # column_to_rownames(var = "ShortSeqID")
# dim(annot_all)
# dim(annot_genes)
```


```{r}
tic()
dge_obj_spot_aloha <- DGEList(counts = all_counts_df,
                              group = c(rep("CA_Catalina_5_May",6),
          rep("CA_PortofLA_5_May",6),
          rep("CA_SPOT_150_May",3),
          rep("CA_SPOT_5_May",12),
          rep("CA_SPOT_890_May",4),
          rep("NPSG_ALOHA_1000_July",1),
          rep("NPSG_ALOHA_1000_March",1),
          rep("NPSG_ALOHA_119_July",2),
          rep("NPSG_ALOHA_120_March",2),
          rep("NPSG_ALOHA_150_July",1),
          rep("NPSG_ALOHA_150_March",1),
          rep("NPSG_ALOHA_5_July",2),
          rep("NPSG_ALOHA_5_March",2)))
toc()
```

```{r}
dge_obj_spot_aloha$samples

tic()
data_tmm <- calcNormFactors(dge_obj_spot_aloha, method="TMM") # TMM normalization
toc() # 1290.48 sec elapsed

class(data_tmm)
# save(data_tmm, file = "../../../scratch/user/skhu/SPOT-ALOHA/tmm.RData")
```


### Checkpoint to load TMM data
```{r}
load(file = "../../../scratch/user/skhu/SPOT-ALOHA/tmm.RData", verbose = TRUE)

data_tmm$samples
```

Get CPM using normalized library size.
```{r}
data_tpm_cpm <- cpm(data_tmm, normalized.lib.sizes = TRUE, log = FALSE)
class(data_tpm_cpm) ## outputs a matrix array

head(data_tpm_cpm)[1:4]
# Transpose
tmp <- t(data_tpm_cpm)
head(tmp)[,1:2]
```
## SUBSET for test data
```{r}
# dim(tmp)
# # 14,841,331 # 14 million
# 
subset_tmp <- tmp[,1:1000000]
# class(subset_tmp)
# head(subset_tmp)[,1:5]
# 
# # Create a mini one too.
# mini_subset_tmp <- tmp[,1:10]
```



# Multidplyr experiment

_This didn't really work_

Using multidplyr to run tidyverse functions in parallel
```{r}
# num_threads
# parallel::detectCores()
# cluster <- new_cluster(num_threads)
# cluster
```

mini first
```{r}
# Isolate samplenames from SRR IDs, send to clusters
# tic()
# metat_cpm_party_SUBSET <- data.frame(mini_subset_tmp) %>%
#   rownames_to_column(var = "SAMPLENAME") %>%
#   separate(SAMPLENAME, into = c("SAMPLE", "RUN"), sep = "-") %>%
#   group_by(SAMPLE) %>%
#     partition(cluster)
# toc() #mini via partition = 4.38 seconds
# 
# tic()
# metat_cpm_SUBSET <- data.frame(mini_subset_tmp) %>%
#   rownames_to_column(var = "SAMPLENAME") %>%
#   separate(SAMPLENAME, into = c("SAMPLE", "RUN"), sep = "-") 
# toc() # without particion = 0.03 seconds
```

```{r}
# head(metat_cpm_party_SUBSET)
# head(metat_cpm_SUBSET)
```


```{r}
# ?pivot_longer()
# ?pivot_longer
# tic()
# metat_cpm <- metat_cpm_party_SUBSET %>% 
#   # pivot_longer(cols = starts_with("Seq_"), names_to = "ShortSeqID_num", values_to = "CPM") %>% 
#   # separate(ShortSeqID_num, into = c("ShortSeqID", "num"), sep = "//.") %>% 
#   group_by(SAMPLE) %>% 
#     # summarise(MEAN_CPM = mean(starts_with("Seq_"))) %>% 
#   summarise_if(is.numeric, mean, na.rm = TRUE) %>% 
#   collect()
# toc() # With mini subset, 10.337 second
# 
# tic()
# metat_cpm <- metat_cpm_SUBSET %>% 
#   group_by(SAMPLE) %>% 
#   summarise_if(is.numeric, mean, na.rm = TRUE)
# toc()
# 
# 
# dim(metat_cpm)
# head(metat_cpm)
# head(annot_all)
# save(metat_cpm, file = "../../../scratch/user/skhu/SPOT-ALOHA/normalized_avg_reps.RData")
```

Multidplyr is not optimized for this work.


# Estimate mean CPM across replicate samples


```{r}
# tmp
# subset_tmp
tic()
metat_mean_cpm <- data.frame(subset_tmp) %>%
  rownames_to_column(var = "SAMPLENAME") %>%
  separate(SAMPLENAME, into = c("SAMPLE", "RUN"), sep = "-") %>%
    group_by(SAMPLE) %>% 
  summarise_if(is.numeric, mean, na.rm = TRUE)
toc()
```


```{r}
dim(tmp)
dim(metat_mean_cpm)

save(metat_mean_cpm, data_tpm_cpm, file = "/scratch/user/skhu/SPOT-ALOHA/mean_cpm_all.Rdata")
```

## Add metadata
```{r}
# ?left_join()
# metat_cpm_wannot <- metat_cpm %>% 
#   left_join(annot_all, )
```

# Repeat above normalization by specific taxonomic groups


```{r}
load(file = "/scratch/user/skhu/SPOT-ALOHA/tmp-may6-2023.RData", verbose = TRUE)
head(counts_annot_reorder)[1:4,1:5]
```

# Query annotation data

```{r}
head(annot_all)
```


```{r}
dim(annot_all)
length(unique(annot_all$ShortSeqID))
14841331 - 14737135

# setdiff((annot_all$ShortSeqID), unique(annot_all$ShortSeqID))

tmp <- annot_all[duplicated(annot_all$ShortSeqID)]
View(annot_all %>% filter(ShortSeqID == "Seq_9995515"))
tmp <- (annot_all %>% filter(ShortSeqID == "Seq_9995515"))
tmp$SequenceID
```
